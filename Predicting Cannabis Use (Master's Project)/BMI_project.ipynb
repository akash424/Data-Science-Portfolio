{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/364 with 171 missing, elapsed time: 0.259\n",
      "Imputing row 101/364 with 5 missing, elapsed time: 0.383\n",
      "Imputing row 201/364 with 68 missing, elapsed time: 0.493\n",
      "Imputing row 301/364 with 8 missing, elapsed time: 0.597\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import CategoricalIndex as cat\n",
    "\n",
    "#--\n",
    "os.chdir(sys.path[0])\n",
    "#os.chdir(\"/Users/Akash\")\n",
    "#os.chdir(\"/../../../../../Volumes/peacd/NEEL\")\n",
    "\n",
    "#--\n",
    "original_data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "#--\n",
    "missing_value_df = pd.DataFrame({'percent_missing': original_data.isnull().sum() * 100 / len(original_data)})\n",
    "missing_value_df.sort_values('percent_missing', inplace=True, ascending=False)\n",
    "\n",
    "#--\n",
    "cols_with_missing_value = missing_value_df[missing_value_df['percent_missing']>72].index.tolist()\n",
    "filtered_data = original_data.drop(columns=cols_with_missing_value)\n",
    "\n",
    "#--\n",
    "variables_list = pd.read_excel(\"Variables_List.xlsx\")\n",
    "\n",
    "#--\n",
    "x_var = []\n",
    "y_var = []\n",
    "for i in range(filtered_data.shape[1]):\n",
    "    index = variables_list.Variable[variables_list.Variable == filtered_data.columns.values[i]].index.astype(int)[0]\n",
    "    if (variables_list['Type'].iloc[index] == \"exp\"):\n",
    "        x_var.append(variables_list.Variable[index])\n",
    "    elif (variables_list['Type'].iloc[index] == \"resp\"):\n",
    "        y_var.append(variables_list.Variable[index])\n",
    "        \n",
    "#--\n",
    "x_filtered_df = filtered_data[x_var]\n",
    "y_filtered_df = filtered_data[y_var]\n",
    "\n",
    "#--\n",
    "catX_var = []\n",
    "numX_var = []\n",
    "for i in range(x_filtered_df.shape[1]):\n",
    "    index = variables_list.Variable[variables_list.Variable == x_filtered_df.columns.values[i]].index.astype(int)[0]\n",
    "    if (variables_list['Class'].iloc[index] == \"nom\" or variables_list['Class'].iloc[index] == \"ord\"):\n",
    "        catX_var.append(variables_list.Variable[index])\n",
    "    elif (variables_list['Class'].iloc[index] == \"num\"):\n",
    "        numX_var.append(variables_list.Variable[index])\n",
    "\n",
    "#--\n",
    "x_filtered_only_cat = x_filtered_df[catX_var]\n",
    "x_filtered_only_num = x_filtered_df[numX_var]\n",
    "\n",
    "#--\n",
    "for col in x_filtered_only_cat.columns.values:\n",
    "    x_filtered_only_cat[col] = x_filtered_only_cat[col].astype('category')\n",
    "\n",
    "col = y_filtered_df.columns.tolist()\n",
    "col.remove(\"g6SUSMAR\")\n",
    "for col in col:\n",
    "    y_filtered_df[col] = y_filtered_df[col].astype('category')\n",
    "    \n",
    "#--\n",
    "x_filtered_only_cat_wide = pd.get_dummies(x_filtered_only_cat)\n",
    "for col in x_filtered_only_cat.columns.values:\n",
    "    x_filtered_only_cat_wide.loc[x_filtered_only_cat[col].isnull(), x_filtered_only_cat_wide.columns.str.startswith(col+\"_\")] = np.nan\n",
    "\n",
    "#--\n",
    "x_filtered_only_cat_wide_minus_col = x_filtered_only_cat_wide.drop(['x6RELAT_3.0','x6RELAT_4.0','x6RELAT_7.0'], 1)\n",
    "x_filtered_only_num_minus_col = x_filtered_only_num.drop('SUUage', 1)\n",
    "\n",
    "#--\n",
    "x_filtered_num_and_cat_minus_col = pd.concat([x_filtered_only_num_minus_col, x_filtered_only_cat_wide_minus_col], 1)\n",
    "\n",
    "#--\n",
    "from fancyimpute import KNN \n",
    "x_filled_knn = pd.DataFrame(KNN(k=3).fit_transform(x_filtered_num_and_cat_minus_col))\n",
    "x_filled_knn.columns = x_filtered_num_and_cat_minus_col.columns.tolist()\n",
    "x_filled_knn.head()\n",
    "\n",
    "#--\n",
    "x_filled_knn.iloc[:,36:350] = round(x_filled_knn.iloc[:,36:350], 0)\n",
    "\n",
    "#--\n",
    "X_final = x_filled_knn.copy()\n",
    "for col in x_filtered_only_num_minus_col.columns.tolist():\n",
    "    colm = x_filled_knn[col].mean()\n",
    "    colstd = x_filled_knn[col].std()\n",
    "    X_final[col] = (x_filled_knn[col] - colm)/colstd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg123(estimator, X, y, target):\n",
    "    training_score = []\n",
    "    crossVal_score = []\n",
    "    testing_score = []\n",
    "    total_num_var = []\n",
    "    max_num_var = []\n",
    "    selected_var = []\n",
    "    \n",
    "    df = pd.concat([X, y], 1)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    if target == 'g6SUUMUSE3':\n",
    "        df = df[df[target]!=5]\n",
    "        df[target].cat.remove_categories([5], inplace=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), \n",
    "                                                        df[target], \n",
    "                                                        stratify=df[target], \n",
    "                                                        test_size=0.15, random_state=0)\n",
    "    \n",
    "    k = len(df[target].unique().tolist())\n",
    "    clf = estimator.fit(X_train, y_train)\n",
    "    \n",
    "    var_per_group = []\n",
    "    num_per_group = []\n",
    "\n",
    "    for j in range(k):\n",
    "        var_per_group.append(X_train.columns.values[list(np.where(clf.coef_[j,:]!=0))])\n",
    "        num_per_group.append(len(X_train.columns.values[list(np.where(clf.coef_[j,:]!=0))]))\n",
    "    \n",
    "    varMax = list(set(np.concatenate(var_per_group).ravel().tolist()))\n",
    "    max_num_var.append(max(num_per_group))\n",
    "    total_num_var.append(len(varMax))\n",
    "    selected_var.append(np.asarray(varMax))\n",
    "    \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "    testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "    crossVal_score.append(np.mean(clf.scores_[0]))\n",
    "    \n",
    "    X_new_train = X_train.loc[:, X_train.columns.isin(varMax)]\n",
    "    X_new_test = X_test.loc[:, X_test.columns.isin(varMax)]\n",
    "    clf = estimator.fit(X_new_train, y_train)\n",
    "    \n",
    "    var_per_group = []\n",
    "    num_per_group = []\n",
    "\n",
    "    for j in range(k):\n",
    "        var_per_group.append(X_new_train.columns.values[list(np.where(clf.coef_[j,:]!=0))])\n",
    "        num_per_group.append(len(X_new_train.columns.values[list(np.where(clf.coef_[j,:]!=0))]))\n",
    "    \n",
    "    varMax = list(set(np.concatenate(var_per_group).ravel().tolist()))\n",
    "    max_num_var.append(max(num_per_group))\n",
    "    total_num_var.append(len(varMax))\n",
    "    selected_var.append(np.asarray(varMax))\n",
    "    \n",
    "    y_pred_train = clf.predict(X_new_train)\n",
    "    y_pred_test = clf.predict(X_new_test)\n",
    "    training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "    testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "    crossVal_score.append(np.mean(clf.scores_[0]))\n",
    "    \n",
    "    diff = crossVal_score[-1] - crossVal_score[-2]\n",
    "    \n",
    "    while diff>0:\n",
    "        X_new_train = X_train.loc[:, X_train.columns.isin(varMax)]\n",
    "        X_new_test = X_test.loc[:, X_test.columns.isin(varMax)]\n",
    "        clf = estimator.fit(X_new_train, y_train)\n",
    "    \n",
    "        var_per_group = []\n",
    "        num_per_group = []\n",
    "\n",
    "        for j in range(k):\n",
    "            var_per_group.append(X_new_train.columns.values[list(np.where(clf.coef_[j,:]!=0))])\n",
    "            num_per_group.append(len(X_new_train.columns.values[list(np.where(clf.coef_[j,:]!=0))]))\n",
    "    \n",
    "        varMax = list(set(np.concatenate(var_per_group).ravel().tolist()))\n",
    "        max_num_var.append(max(num_per_group))\n",
    "        total_num_var.append(len(varMax))\n",
    "        selected_var.append(np.asarray(varMax))\n",
    "    \n",
    "        y_pred_train = clf.predict(X_new_train)\n",
    "        y_pred_test = clf.predict(X_new_test)\n",
    "        training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "        testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "        crossVal_score.append(np.mean(clf.scores_[0]))\n",
    "    \n",
    "        diff = crossVal_score[-1] - crossVal_score[-2]\n",
    "        \n",
    "    \n",
    "    if crossVal_score[-1] == max(crossVal_score):\n",
    "        which_model = len(crossVal_score) - 1\n",
    "    else:\n",
    "        which_model = crossVal_score.index(max(crossVal_score)) \n",
    "    \n",
    "    print(\"Model #\",str(which_model+1),\"is selected\",\n",
    "          \"\\n\\nThe total number of variables is:\\t\",\n",
    "          len(selected_var[which_model]),\"\\n\\nThese are the variables:\\n\\n\",\n",
    "          selected_var[which_model],\"\\n\\nMost number of variables needed to predict one of the classes in each iteration:\\n\",\n",
    "          max_num_var,\"\\n\\nTraining accuracy in each iteration:\\n\",\n",
    "          training_score,\"\\n\\nCross-validation accuracy in each iteration:\\n\",\n",
    "          crossVal_score,\"\\n\\nTesting accuracy in each iteration:\\n\",\n",
    "          testing_score)\n",
    "    \n",
    "    global final_var\n",
    "    final_var = selected_var[which_model]\n",
    "    \n",
    "    global train_acc\n",
    "    train_acc = training_score[which_model]\n",
    "    \n",
    "    global test_acc\n",
    "    test_acc = testing_score[which_model]\n",
    "    \n",
    "    global val_acc\n",
    "    val_acc = crossVal_score[which_model]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogRegMJ(estimator, X, y, target):\n",
    "    training_score = []\n",
    "    crossVal_score = []\n",
    "    testing_score = []\n",
    "    total_num_var = []\n",
    "    max_num_var = []\n",
    "    selected_var = []\n",
    "    \n",
    "    df = pd.concat([X, y], 1)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), \n",
    "                                                        df[target], \n",
    "                                                        stratify=df[target], \n",
    "                                                        test_size=0.15, random_state=0)\n",
    "    \n",
    "    k = len(df[target].unique().tolist())\n",
    "    clf = estimator.fit(X_train, y_train)\n",
    "    \n",
    "    var_per_group = []\n",
    "    num_per_group = []\n",
    "\n",
    "    var_per_group.append(X_train.columns.values[list(np.where(clf.coef_[0,:]!=0))])\n",
    "    num_per_group.append(len(X_train.columns.values[list(np.where(clf.coef_[0,:]!=0))]))\n",
    "    \n",
    "    varMax = list(set(np.concatenate(var_per_group).ravel().tolist()))\n",
    "    max_num_var.append(max(num_per_group))\n",
    "    total_num_var.append(len(varMax))\n",
    "    selected_var.append(np.asarray(varMax))\n",
    "    \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "    testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "    crossVal_score.append(np.mean(list(clf.scores_.values())))\n",
    "    \n",
    "    X_new_train = X_train.loc[:, X_train.columns.isin(varMax)]\n",
    "    X_new_test = X_test.loc[:, X_test.columns.isin(varMax)]\n",
    "    clf = estimator.fit(X_new_train, y_train)\n",
    "    \n",
    "    var_per_group = []\n",
    "    num_per_group = []\n",
    "\n",
    "    var_per_group.append(X_new_train.columns.values[list(np.where(clf.coef_[0,:]!=0))])\n",
    "    num_per_group.append(len(X_new_train.columns.values[list(np.where(clf.coef_[0,:]!=0))]))\n",
    "    \n",
    "    varMax = list(set(np.concatenate(var_per_group).ravel().tolist()))\n",
    "    max_num_var.append(max(num_per_group))\n",
    "    total_num_var.append(len(varMax))\n",
    "    selected_var.append(np.asarray(varMax))\n",
    "    \n",
    "    y_pred_train = clf.predict(X_new_train)\n",
    "    y_pred_test = clf.predict(X_new_test)\n",
    "    training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "    testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "    crossVal_score.append(np.mean(list(clf.scores_.values())))\n",
    "    \n",
    "    diff = crossVal_score[-1] - crossVal_score[-2]\n",
    "    \n",
    "    while diff>0:\n",
    "        X_new_train = X_train.loc[:, X_train.columns.isin(varMax)]\n",
    "        X_new_test = X_test.loc[:, X_test.columns.isin(varMax)]\n",
    "        clf = estimator.fit(X_new_train, y_train)\n",
    "    \n",
    "        var_per_group = []\n",
    "        num_per_group = []\n",
    "\n",
    "        var_per_group.append(X_new_train.columns.values[list(np.where(clf.coef_[0,:]!=0))])\n",
    "        num_per_group.append(len(X_new_train.columns.values[list(np.where(clf.coef_[0,:]!=0))]))\n",
    "    \n",
    "        varMax = list(set(np.concatenate(var_per_group).ravel().tolist()))\n",
    "        max_num_var.append(max(num_per_group))\n",
    "        total_num_var.append(len(varMax))\n",
    "        selected_var.append(np.asarray(varMax))\n",
    "    \n",
    "        y_pred_train = clf.predict(X_new_train)\n",
    "        y_pred_test = clf.predict(X_new_test)\n",
    "        training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "        testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "        crossVal_score.append(np.mean(list(clf.scores_.values())))\n",
    "    \n",
    "        diff = crossVal_score[-1] - crossVal_score[-2]\n",
    "\n",
    "    \n",
    "    if crossVal_score[-1] == max(crossVal_score):\n",
    "        which_model = len(crossVal_score) - 1\n",
    "    else:\n",
    "        which_model = crossVal_score.index(max(crossVal_score)) \n",
    "    \n",
    "    print(\"Model #\",str(which_model+1),\"is selected\",\n",
    "          \"\\n\\nThe total number of variables is:\\t\",\n",
    "          len(selected_var[which_model]),\"\\n\\nThese are the variables:\\n\\n\",\n",
    "          selected_var[which_model],\"\\n\\nMost number of variables needed to predict one of the classes in each iteration:\\n\",\n",
    "          max_num_var,\"\\n\\nTraining accuracy in each iteration:\\n\",\n",
    "          training_score,\"\\n\\nCross-validation accuracy in each iteration:\\n\",\n",
    "          crossVal_score,\"\\n\\nTesting accuracy in each iteration:\\n\",\n",
    "          testing_score)\n",
    "    \n",
    "    global final_var\n",
    "    final_var = selected_var[which_model]\n",
    "    \n",
    "    global train_acc\n",
    "    train_acc = training_score[which_model]\n",
    "    \n",
    "    global test_acc\n",
    "    test_acc = testing_score[which_model]\n",
    "    \n",
    "    global val_acc\n",
    "    val_acc = crossVal_score[which_model]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 2 is selected \n",
      "\n",
      "The total number of variables is:\t 124 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['x6mcanp2' 'x6RELAT_1.0' 'EDUC_8.0' 'x6RELAT_2.0' 'MomDiscuss'\n",
      " 'x6w6MUSE1_6.0' 'c6w6use_0.0' 'g6cage' 'DadNegExp' 'c6mcanp2'\n",
      " 'g6fPSTR13_5.0' 'c6use_0.0' 'x6w6MUSE1_0.0' 'g6mPSTR15_2.0' 'g2agepar'\n",
      " 'c6w4muse2_2.0' 'x6w6MUSE1_2.0' 'g6fparmon' 'c6w4muse1_5.0'\n",
      " 'g6fPSTR12_1.0' 'g6mPSTR10_1.0' 'g6GEN_1.0' 'g6ETHNIC_2.0' 'x6mcanp05'\n",
      " 'g6fPSTR9_5.0' 'g6mpstrmar' 'EDUC_m_4.0' 'ParHist_1.0' 'x6w6MUSE1_7.0'\n",
      " 'g6ETHNIC_1.0' 'c6w5muse1_7.0' 'c6w5canre_6.0' 'EDUC_m_5.0'\n",
      " 'c6w6MUSE2_0.0' 'x6w6use_1.0' 'x6dep_0.0' 'EDUC_7.0' 'g6mPSTR10_3.0'\n",
      " 'g6fPSTR13_1.0' 'EDUC_9.0' 'g6ETHNIC_6.0' 'x6w6can4d' 'g6mcanp1'\n",
      " 'c6mcanp01' 'g6fPSTR11_1.0' 'g6mPSTR16_1.0' 'g6fPSTR9_1.0' 'EDUC_m_3.0'\n",
      " 'g6mparcon' 'g6SUUcage' 'x6w6can4a' 'EDUC_d_7.0' 'g6fPSTR12_4.0'\n",
      " 'c6w6MUSE1_0.0' 'g6fPSTR11_2.0' 'g6fPSTR11_4.0' 'x6mcanp01' 'g6mparmon'\n",
      " 'c6mcanp1' 'c6w6MUSE2_4.0' 'c6mcanp05' 'g6mPSTR14_1.0' 'EDUC_11.0'\n",
      " 'g6fparsup' 'g6mPSTR14_5.0' 'MomPrevent' 'x6dep_1.0' 'g6mcanp01'\n",
      " 'g6GEN_2.0' 'g6mparsup' 'EDUC_d_9.0' 'ethnic_1.0' 'c6w6can4d'\n",
      " 'AUD_3Level_0.0' 'g6mcanp05' 'g6mPSTR9_3.0' 'c6w5muse2_0.0'\n",
      " 'AUD_3Level_1.0' 'c6w5muse1_3.0' 'g6fparcon' 'MomNegExp' 'g6mPSTR11_3.0'\n",
      " 'g6fPSTR11_3.0' 'x6use_1.0' 'g6mPSTR12_3.0' 'g6mPSTR15_1.0'\n",
      " 'g6mPSTR10_5.0' 'x6w6MUSE2_6.0' 'g6mPSTR16_2.0' 'g6mcanp2'\n",
      " 'g6mPSTR14_3.0' 'g6mPSTR13_3.0' 'x6use_0.0' 'g6fPSTR16_3.0' 'DadDiscuss'\n",
      " 'g6fPSTR15_5.0' 'EDUC_d_4.0' 'g6fPSTR16_1.0' 'c6w4muse1_1.0'\n",
      " 'c6w4canre_-1.0' 'g6mPSTR11_4.0' 'c6use_1.0' 'g6fPSTR15_1.0'\n",
      " 'g6fPSTR12_5.0' 'x6w6use_0.0' 'c6w5muse2_4.0' 'x6w6MUSE1_1.0'\n",
      " 'c6w4canon_-1.0' 'g6fPSTR9_3.0' 'c6cage' 'g6fPSTR10_5.0' 'g6fPSTR12_2.0'\n",
      " 'c6w5use_1.0' 'c6RELAT_2.0' 'g6mPSTR9_2.0' 'c6w5use_0.0' 'c6w6can4a'\n",
      " 'x6mcanp1' 'c6w6use_1.0' 'g6fPSTR13_3.0' 'g6mPSTR12_4.0' 'DadPrevent'\n",
      " 'c6w5muse1_0.0' 'g6fPSTR14_3.0'] \n",
      "\n",
      "Most number of variables needed to predict one of the classes in each iteration:\n",
      " [65, 66, 66] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.9446254071661238, 0.9446254071661238, 0.9446254071661238] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.7136196166293254, 0.7267587752053771, 0.7234254418720439] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.6909090909090909, 0.6909090909090909, 0.6909090909090909]\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegressionCV(solver='saga',penalty='l1', Cs=[1], max_iter = 10000, scoring='accuracy',\n",
    "                   multi_class='multinomial',random_state=0, cv=3, refit=True)\n",
    "X = X_final\n",
    "y = y_filtered_df['g6SUUMUSE1']\n",
    "target = 'g6SUUMUSE1'\n",
    "\n",
    "LogReg123(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_use_predictors_regression' (ndarray)\n",
      "Stored 'max_use_scores_regression' (list)\n"
     ]
    }
   ],
   "source": [
    "max_use_predictors_regression = final_var\n",
    "max_use_scores_regression = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store max_use_predictors_regression\n",
    "%store max_use_scores_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past year use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 1 is selected \n",
      "\n",
      "The total number of variables is:\t 103 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['x6mcanp2' 'x6RELAT_1.0' 'c6w4muse1_6.0' 'MomDiscuss' 'c6w6use_0.0'\n",
      " 'c6w4use_0.0' 'g6fpstrmar' 'g6cage' 'EDUC_d_3.0' 'DadNegExp' 'c6mcanp2'\n",
      " 'g6fVALMAR_7.0' 'g6fPSTR13_5.0' 'x6w6MUSE2_0.0' 'EDUC_3.0'\n",
      " 'g6fPSTR10_3.0' 'g6mPSTR14_2.0' 'x6w6MUSE1_2.0' 'g6fparmon'\n",
      " 'g6fPSTR9_2.0' 'c6w6candep_1.0' 'g6fPSTR12_1.0' 'g6mPSTR10_1.0'\n",
      " 'g6GEN_1.0' 'g6ETHNIC_2.0' 'x6mcanp05' 'g6fPSTR9_5.0' 'g6mpstrmar'\n",
      " 'ParHist_1.0' 'EDUC_m_4.0' 'c6w5canre_6.0' 'c6w6MUSE2_0.0' 'x6dep_0.0'\n",
      " 'EDUC_7.0' 'c6w5muse1_1.0' 'x6w6can4d' 'c6mcanp01' 'g6mcanp1'\n",
      " 'g6mPSTR16_1.0' 'g6mparcon' 'g6SUUcage' 'x6w6can4a' 'g6mPSTR9_5.0'\n",
      " 'g6fPSTR12_4.0' 'EDUC_d_7.0' 'c6w6MUSE1_0.0' 'c6w4muse1_7.0'\n",
      " 'g6fPSTR11_4.0' 'x6mcanp01' 'g6mparmon' 'c6mcanp1' 'c6mcanp05'\n",
      " 'c6w4use_1.0' 'g6fparsup' 'c6w4muse1_0.0' 'c6w6MUSE1_1.0' 'g6GEN_2.0'\n",
      " 'MomPrevent' 'g6mcanp01' 'g6mPSTR14_5.0' 'g6mparsup' 'EDUC_d_9.0'\n",
      " 'c6w6can4d' 'AUD_3Level_0.0' 'g6mcanp05' 'c6w5muse2_0.0' 'AUD_3Level_1.0'\n",
      " 'c6w5muse1_3.0' 'g6fparcon' 'c6RELAT_1.0' 'MomNegExp' 'g6mPSTR11_3.0'\n",
      " 'g6mPSTR12_3.0' 'g6mPSTR15_1.0' 'g6mPSTR10_5.0' 'g6mPSTR16_2.0'\n",
      " 'c6w5muse3_0.0' 'g6fPSTR10_1.0' 'g6mcanp2' 'g6mPSTR14_3.0'\n",
      " 'g6fPSTR16_3.0' 'EDUC_d_4.0' 'g6mPSTR11_4.0' 'c6w5muse3_3.0'\n",
      " 'g6fPSTR15_1.0' 'g6fPSTR12_5.0' 'c6w6candep_0.0' 'x6w6MUSE1_1.0' 'c6cage'\n",
      " 'g6fPSTR12_2.0' 'EDUC_m_7.0' 'c6w5use_1.0' 'c6RELAT_2.0' 'c6RELAT_6.0'\n",
      " 'g6mPSTR16_3.0' 'c6w5use_0.0' 'c6w6can4a' 'x6mcanp1' 'c6w6use_1.0'\n",
      " 'g6fPSTR13_3.0' 'g6mPSTR12_4.0' 'DadPrevent' 'c6w5muse1_0.0'] \n",
      "\n",
      "Most number of variables needed to predict one of the classes in each iteration:\n",
      " [48, 48] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.9411764705882353, 0.9411764705882353] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.8236099324218137, 0.8070422756561371] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.8363636363636363, 0.8363636363636363]\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegressionCV(solver='saga',penalty='l1', Cs=[1], max_iter = 10000, scoring='accuracy',\n",
    "                   multi_class='multinomial',random_state=0, cv=3, refit=True)\n",
    "X = X_final\n",
    "y = y_filtered_df['g6SUUMUSE2']\n",
    "target = 'g6SUUMUSE2'\n",
    "\n",
    "LogReg123(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pastYear_use_predictors_regression' (ndarray)\n",
      "Stored 'pastYear_use_scores_regression' (list)\n"
     ]
    }
   ],
   "source": [
    "pastYear_use_predictors_regression = final_var\n",
    "pastYear_use_scores_regression = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store pastYear_use_predictors_regression\n",
    "%store pastYear_use_scores_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past 3 months use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 3 is selected \n",
      "\n",
      "The total number of variables is:\t 70 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['g6mPSTR12_5.0' 'g6fPSTR16_3.0' 'g6mPSTR14_1.0' 'EDUC_11.0' 'g6fparsup'\n",
      " 'x6mcanp2' 'g6ETHNIC_1.0' 'c6dep_0.0' 'MomPrevent' 'c6w4canre_-1.0'\n",
      " 'MomDiscuss' 'g6mPSTR11_4.0' 'g6mparsup' 'g6mcanp01' 'x6w6MUSE1_6.0'\n",
      " 'DadPrevent' 'c6w5muse3_3.0' 'g6fPSTR14_5.0' 'c6w6use_0.0' 'ethnic_1.0'\n",
      " 'c6dep_1.0' 'c6w6can4d' 'x6w6MUSE1_5.0' 'x6w6use_1.0' 'g6cage'\n",
      " 'g6fPSTR12_5.0' 'g6mcanp05' 'DadNegExp' 'c6mcanp2' 'EDUC_6.0'\n",
      " 'g6mPSTR10_3.0' 'g6mPSTR9_3.0' 'AUD_3Level_1.0' 'x6w6MUSE2_0.0'\n",
      " 'c6w4canon_-1.0' 'g6fPSTR9_3.0' 'c6cage' 'x6w6can4d' 'g6mcanp1'\n",
      " 'c6mcanp01' 'g6fparcon' 'g6fPSTR11_1.0' 'c6RELAT_1.0' 'MomNegExp'\n",
      " 'g6fPSTR10_4.0' 'g6mparcon' 'g6SUUcage' 'x6w6can4a' 'c6w4candx_4.0'\n",
      " 'x6w6MUSE3_1.0' 'g6fPSTR13_4.0' 'g6mPSTR13_1.0' 'EDUC_d_7.0' 'g6fparmon'\n",
      " 'c6w6MUSE1_0.0' 'x6use_1.0' 'g6fPSTR9_2.0' 'g6mPSTR15_1.0'\n",
      " 'g6mPSTR10_5.0' 'c6w6can4a' 'EDUC_m_8.0' 'c6w5muse3_0.0' 'c6w6use_1.0'\n",
      " 'g6fPSTR13_2.0' 'g6fPSTR10_1.0' 'g6mcanp2' 'x6mcanp01' 'g6mparmon'\n",
      " 'c6mcanp1' 'x6mcanp05'] \n",
      "\n",
      "Most number of variables needed to predict one of the classes in each iteration:\n",
      " [42, 42, 42] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.9673202614379085, 0.9673202614379085, 0.9673202614379085] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.8725699636829738, 0.8824182866908781, 0.8824182866908781] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.8703703703703703, 0.8703703703703703, 0.8703703703703703]\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegressionCV(solver='saga',penalty='l1', Cs=[1], max_iter = 10000, scoring='accuracy',\n",
    "                   multi_class='multinomial',random_state=0, cv=2, refit=True)\n",
    "X = X_final\n",
    "y = y_filtered_df['g6SUUMUSE3']\n",
    "target = 'g6SUUMUSE3'\n",
    "\n",
    "LogReg123(estimator, X, y, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'past3mo_use_predictors_regression' (ndarray)\n",
      "Stored 'past3mo_use_scores_regression' (list)\n"
     ]
    }
   ],
   "source": [
    "past3mo_use_predictors_regression = final_var\n",
    "past3mo_use_scores_regression = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store past3mo_use_predictors_regression\n",
    "%store past3mo_use_scores_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General use (yes or no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 3 is selected \n",
      "\n",
      "The total number of variables is:\t 75 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['g6mPSTR12_5.0' 'DadDiscuss' 'EDUC_d_4.0' 'g6fparsup' 'x6mcanp2'\n",
      " 'x6RELAT_1.0' 'g6ETHNIC_1.0' 'x6w6MUSE1_7.0' 'MomPrevent' 'g6mcanp01'\n",
      " 'g6mPSTR11_4.0' 'g6mPSTR15_3.0' 'EDUC_d_9.0' 'c6w5muse3_3.0'\n",
      " 'g6fPSTR14_3.0' 'c6w6use_0.0' 'ethnic_1.0' 'c6w5candx_4.0' 'g6cage'\n",
      " 'x6w6MUSE1_5.0' 'EDUC_d_3.0' 'AUD_3Level_0.0' 'g6mcanp05' 'DadNegExp'\n",
      " 'g6fPSTR12_5.0' 'c6mcanp2' 'EDUC_7.0' 'g6mPSTR10_3.0' 'c6w5muse2_0.0'\n",
      " 'g6mPSTR11_2.0' 'c6w5muse1_2.0' 'g6ETHNIC_6.0' 'g6fPSTR9_3.0' 'c6cage'\n",
      " 'x6w6can4d' 'g6fPSTR10_5.0' 'g6mPSTR15_2.0' 'g6mcanp1' 'c6mcanp01'\n",
      " 'g6fparcon' 'g6fPSTR11_1.0' 'g6mPSTR16_1.0' 'g6fPSTR10_3.0'\n",
      " 'g6fPSTR12_2.0' 'g6mparcon' 'g6SUUcage' 'x6w6can4a' 'g6mPSTR11_3.0'\n",
      " 'g6mPSTR12_1.0' 'g6mPSTR9_5.0' 'c6w4muse2_2.0' 'EDUC_m_7.0' 'c6w5use_1.0'\n",
      " 'EDUC_d_1.0' 'x6w6MUSE1_2.0' 'EDUC_d_7.0' 'c6w6MUSE1_0.0' 'g6fPSTR11_3.0'\n",
      " 'x6w6MUSE2_6.0' 'c6w4muse1_5.0' 'g6fPSTR12_1.0' 'g6mPSTR16_2.0'\n",
      " 'c6w5use_0.0' 'c6w6can4a' 'g6fPSTR14_1.0' 'x6mcanp1' 'c6w6use_1.0'\n",
      " 'g6mPSTR12_4.0' 'g6fPSTR10_1.0' 'x6mcanp01' 'g6mparmon' 'x6mcanp05'\n",
      " 'c6w5muse1_0.0' 'g6fPSTR12_3.0' 'c6mcanp05'] \n",
      "\n",
      "Most number of variables needed to predict one of the classes in each iteration:\n",
      " [76, 75, 75] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.9543973941368078, 0.9543973941368078, 0.9543973941368078] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.8565157486622449, 0.8630523246499407, 0.8630523246499407] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.8363636363636363, 0.8363636363636363, 0.8363636363636363]\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegressionCV(solver='saga',penalty='l1', Cs=[1], max_iter = 10000, scoring='accuracy',\n",
    "                   multi_class='multinomial',random_state=0, cv=3, refit=True)\n",
    "X = X_final\n",
    "y = y_filtered_df['MJ_Outcome_YesNo']\n",
    "target = 'MJ_Outcome_YesNo'\n",
    "\n",
    "LogRegMJ(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'gen_use_predictors_regression' (ndarray)\n",
      "Stored 'gen_use_scores_regression' (list)\n"
     ]
    }
   ],
   "source": [
    "gen_use_predictors_regression = final_var\n",
    "gen_use_scores_regression = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store gen_use_predictors_regression\n",
    "%store gen_use_scores_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandFor(estimator, X, y, target):\n",
    "    training_score = []\n",
    "    crossVal_score = []\n",
    "    testing_score = []\n",
    "    total_num_var = []\n",
    "    selected_param = []\n",
    "    selected_var = []\n",
    "    \n",
    "    df = pd.concat([X, y], 1)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    if target == 'g6SUUMUSE3':\n",
    "        df = df[df[target]!=5]\n",
    "        df[target].cat.remove_categories([5], inplace=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), \n",
    "                                                        df[target], \n",
    "                                                        stratify=df[target], \n",
    "                                                        test_size=0.15, random_state=0)\n",
    "    \n",
    "    params = {'n_estimators':[2,5,10,50,100,200,500],\n",
    "              'min_samples_leaf':[.01, .05, .1, .2]}\n",
    "    \n",
    "    modSearch = GridSearchCV(estimator, params, cv=2, error_score=np.nan)\n",
    "    modSearch.fit(X_train, y_train)\n",
    "    rf = modSearch.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "    var = X_train.columns.values[list(np.where(rf.feature_importances_!=0))]\n",
    "    total_num_var.append(len(var))\n",
    "    selected_param.append(modSearch.best_params_)\n",
    "    selected_var.append(np.asarray(var))\n",
    "    \n",
    "    y_pred_train = rf.predict(X_train)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "    training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "    testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "    crossVal_score.append(modSearch.best_score_)\n",
    "    \n",
    "    X_new_train = X_train.loc[:, X_train.columns.isin(var)]\n",
    "    X_new_test = X_test.loc[:, X_test.columns.isin(var)]\n",
    "    modSearch.fit(X_new_train, y_train)\n",
    "    rf = modSearch.best_estimator_.fit(X_new_train, y_train)\n",
    "    \n",
    "    var = X_new_train.columns.values[list(np.where(rf.feature_importances_!=0))]\n",
    "    total_num_var.append(len(var))\n",
    "    selected_param.append(modSearch.best_params_)\n",
    "    selected_var.append(np.asarray(var))\n",
    "    \n",
    "    y_pred_train = rf.predict(X_new_train)\n",
    "    y_pred_test = rf.predict(X_new_test)\n",
    "    training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "    testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "    crossVal_score.append(modSearch.best_score_)\n",
    "    \n",
    "    diff = crossVal_score[-1] - crossVal_score[-2]\n",
    "    \n",
    "    while diff>0:\n",
    "        X_new_train = X_train.loc[:, X_train.columns.isin(var)]\n",
    "        X_new_test = X_test.loc[:, X_test.columns.isin(var)]\n",
    "        modSearch.fit(X_new_train, y_train)\n",
    "        rf = modSearch.best_estimator_.fit(X_new_train, y_train)\n",
    "    \n",
    "        var = X_new_train.columns.values[list(np.where(rf.feature_importances_!=0))]\n",
    "        total_num_var.append(len(var))\n",
    "        selected_param.append(modSearch.best_params_)\n",
    "        selected_var.append(np.asarray(var))\n",
    "    \n",
    "        y_pred_train = rf.predict(X_new_train)\n",
    "        y_pred_test = rf.predict(X_new_test)\n",
    "        training_score.append(accuracy_score(y_train, y_pred_train))\n",
    "        testing_score.append(accuracy_score(y_test, y_pred_test))\n",
    "        crossVal_score.append(modSearch.best_score_)\n",
    "\n",
    "        diff = crossVal_score[-1] - crossVal_score[-2]\n",
    "\n",
    "    \n",
    "    if total_num_var.count(min(total_num_var))>1:\n",
    "        which_model = testing_score.index(max(testing_score))\n",
    "    elif crossVal_score[-1] == max(crossVal_score) or diff > -0.01:\n",
    "        which_model = len(crossVal_score) - 1\n",
    "    else:\n",
    "        which_model = crossVal_score.index(max(crossVal_score))\n",
    "    \n",
    "    print(\"Model #\",str(which_model+1),\"is selected\",\n",
    "          \"\\n\\nFinal model parameters:\\t\",\n",
    "          selected_param[which_model],\"\\n\\nThe total number of variables is:\\t\",\n",
    "          len(selected_var[which_model]),\"\\n\\nThese are the variables:\\n\\n\",\n",
    "          selected_var[which_model],\"\\n\\nNumber of variables in each iteration:\\n\",\n",
    "          total_num_var,\"\\n\\nTraining accuracy in each iteration:\\n\",\n",
    "          training_score,\"\\n\\nCross-validation accuracy in each iteration:\\n\",\n",
    "          crossVal_score,\"\\n\\nTesting accuracy in each iteration:\\n\",\n",
    "          testing_score)\n",
    "    \n",
    "    global final_var\n",
    "    final_var = selected_var[which_model]\n",
    "    \n",
    "    global train_acc\n",
    "    train_acc = training_score[which_model]\n",
    "    \n",
    "    global test_acc\n",
    "    test_acc = testing_score[which_model]\n",
    "    \n",
    "    global val_acc\n",
    "    val_acc = crossVal_score[which_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 2 is selected \n",
      "\n",
      "Final model parameters:\t {'min_samples_leaf': 0.01, 'n_estimators': 50} \n",
      "\n",
      "The total number of variables is:\t 168 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['g6SUUcage' 'g6cage' 'g6fpstrmar' 'g6mpstrmar' 'c6w6can4d' 'c6w6can4a'\n",
      " 'x6w6can4d' 'x6w6can4a' 'par_fscore' 'parf_score2' 'MomPrevent'\n",
      " 'MomDiscuss' 'MomNegExp' 'DadPrevent' 'DadDiscuss' 'DadNegExp'\n",
      " 'g6fparsup' 'g6fparcon' 'g6fparmon' 'g6mparsup' 'g6mparcon' 'g6mparmon'\n",
      " 'c6cage' 'g2agepar' 'g6mcanp01' 'g6mcanp05' 'g6mcanp1' 'g6mcanp2'\n",
      " 'x6mcanp01' 'x6mcanp05' 'x6mcanp1' 'x6mcanp2' 'c6mcanp01' 'c6mcanp05'\n",
      " 'c6mcanp1' 'c6mcanp2' 'c6RELAT_1.0' 'c6RELAT_2.0' 'x6RELAT_1.0'\n",
      " 'x6RELAT_2.0' 'g6GEN_1.0' 'g6GEN_2.0' 'g6ETHNIC_1.0' 'g6ETHNIC_2.0'\n",
      " 'g6fPSTR9_1.0' 'g6fPSTR9_3.0' 'g6fPSTR9_5.0' 'g6fPSTR10_1.0'\n",
      " 'g6fPSTR10_3.0' 'g6fPSTR10_4.0' 'g6fPSTR10_5.0' 'g6fPSTR11_1.0'\n",
      " 'g6fPSTR11_3.0' 'g6fPSTR11_4.0' 'g6fPSTR11_5.0' 'g6fPSTR12_1.0'\n",
      " 'g6fPSTR12_2.0' 'g6fPSTR12_4.0' 'g6fPSTR12_5.0' 'g6fPSTR13_1.0'\n",
      " 'g6fPSTR13_3.0' 'g6fPSTR13_4.0' 'g6fPSTR13_5.0' 'g6fPSTR14_1.0'\n",
      " 'g6fPSTR14_2.0' 'g6fPSTR14_3.0' 'g6fPSTR14_4.0' 'g6fPSTR14_5.0'\n",
      " 'g6fPSTR15_1.0' 'g6fPSTR16_1.0' 'g6fVALMAR_7.0' 'g6mPSTR9_2.0'\n",
      " 'g6mPSTR9_3.0' 'g6mPSTR9_4.0' 'g6mPSTR9_5.0' 'g6mPSTR10_1.0'\n",
      " 'g6mPSTR10_5.0' 'g6mPSTR11_2.0' 'g6mPSTR11_3.0' 'g6mPSTR11_4.0'\n",
      " 'g6mPSTR11_5.0' 'g6mPSTR12_1.0' 'g6mPSTR12_2.0' 'g6mPSTR12_5.0'\n",
      " 'g6mPSTR13_1.0' 'g6mPSTR13_2.0' 'g6mPSTR13_3.0' 'g6mPSTR13_4.0'\n",
      " 'g6mPSTR13_5.0' 'g6mPSTR14_1.0' 'g6mPSTR14_2.0' 'g6mPSTR14_5.0'\n",
      " 'g6mPSTR15_1.0' 'g6mPSTR15_4.0' 'g6mPSTR16_1.0' 'g6mPSTR16_2.0'\n",
      " 'c6w4muse1_0.0' 'c6w4muse1_1.0' 'c6w4muse1_2.0' 'c6w4muse1_7.0'\n",
      " 'c6w4muse2_0.0' 'c6w4muse2_1.0' 'c6w4muse2_2.0' 'c6w4candx_1.0'\n",
      " 'c6w4canon_-1.0' 'c6w4canon_0.0' 'c6w4canre_-1.0' 'c6w5muse1_0.0'\n",
      " 'c6w5muse1_2.0' 'c6w5muse1_3.0' 'c6w5muse1_7.0' 'c6w5canon_0.0'\n",
      " 'c6w5canon_6.0' 'c6w6MUSE1_0.0' 'c6w6MUSE1_2.0' 'c6w6MUSE1_7.0'\n",
      " 'c6w6MUSE2_0.0' 'c6w6MUSE3_0.0' 'c6w6MUSE3_1.0' 'x6w6MUSE1_0.0'\n",
      " 'x6w6MUSE1_1.0' 'x6w6MUSE1_2.0' 'x6w6MUSE1_4.0' 'x6w6MUSE1_5.0'\n",
      " 'x6w6MUSE2_0.0' 'x6w6MUSE2_6.0' 'x6w6MUSE3_0.0' 'c6w6candep_1.0'\n",
      " 'x6w6candep_0.0' 'x6w6candep_1.0' 'c6dep_0.0' 'c6dep_1.0' 'x6can4_1.0'\n",
      " 'x6dep_0.0' 'x6dep_1.0' 'w4parentdep_1.0' 'w5parentdep_0.0'\n",
      " 'parentdx_0.0' 'parentdx_1.0' 'c6w4use_0.0' 'c6w4use_1.0' 'c6w5use_0.0'\n",
      " 'c6w5use_1.0' 'c6w6use_0.0' 'c6w6use_1.0' 'x6w6use_0.0' 'x6w6use_1.0'\n",
      " 'c6use_0.0' 'c6use_1.0' 'x6use_0.0' 'x6use_1.0' 'parentuse_1.0'\n",
      " 'ParHist_0.0' 'ParHist_1.0' 'ParHist_2.0' 'ethnic_1.0' 'ethnic_2.0'\n",
      " 'EDUC_3.0' 'EDUC_4.0' 'EDUC_7.0' 'EDUC_9.0' 'EDUC_m_7.0' 'EDUC_d_3.0'\n",
      " 'EDUC_d_7.0' 'AUD_3Level_0.0' 'AUD_3Level_1.0' 'w6parentdep_0.0'\n",
      " 'w6parentdep_1.0'] \n",
      "\n",
      "Number of variables in each iteration:\n",
      " [195, 168] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.993485342019544, 0.9869706840390879] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.7719869706840391, 0.7719869706840391] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.7272727272727273, 0.7272727272727273]\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestClassifier(random_state=0, bootstrap=True, oob_score=True,\n",
    "                                   class_weight={0:1,1:10,2:15,3:15,4:15,5:15,6:15,7:15})\n",
    "X = x_filled_knn\n",
    "y = y_filtered_df['g6SUUMUSE1']\n",
    "target = 'g6SUUMUSE1'\n",
    "\n",
    "RandFor(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_use_predictors_tree' (ndarray)\n",
      "Stored 'max_use_scores_tree' (list)\n"
     ]
    }
   ],
   "source": [
    "max_use_predictors_tree = final_var\n",
    "max_use_scores_tree = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store max_use_predictors_tree\n",
    "%store max_use_scores_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past year use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 1 is selected \n",
      "\n",
      "Final model parameters:\t {'min_samples_leaf': 0.1, 'n_estimators': 100} \n",
      "\n",
      "The total number of variables is:\t 97 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['g6SUUcage' 'g6cage' 'g6fpstrmar' 'g6mpstrmar' 'c6w6can4d' 'x6w6can4d'\n",
      " 'par_fscore' 'parf_score2' 'MomPrevent' 'MomDiscuss' 'MomNegExp'\n",
      " 'DadPrevent' 'DadDiscuss' 'DadNegExp' 'g6fparsup' 'g6fparcon' 'g6fparmon'\n",
      " 'g6mparsup' 'g6mparcon' 'g6mparmon' 'c6cage' 'g2agepar' 'g6mcanp01'\n",
      " 'g6mcanp05' 'g6mcanp1' 'g6mcanp2' 'x6mcanp01' 'x6mcanp05' 'x6mcanp1'\n",
      " 'x6mcanp2' 'c6mcanp01' 'c6mcanp05' 'c6mcanp1' 'c6mcanp2' 'c6RELAT_2.0'\n",
      " 'x6RELAT_1.0' 'x6RELAT_2.0' 'g6GEN_1.0' 'g6ETHNIC_1.0' 'g6fPSTR9_1.0'\n",
      " 'g6fPSTR10_5.0' 'g6fPSTR11_1.0' 'g6fPSTR11_5.0' 'g6fPSTR12_1.0'\n",
      " 'g6fPSTR14_1.0' 'g6fPSTR15_1.0' 'g6fPSTR16_1.0' 'g6mPSTR9_1.0'\n",
      " 'g6mPSTR10_1.0' 'g6mPSTR10_5.0' 'g6mPSTR11_1.0' 'g6mPSTR11_3.0'\n",
      " 'g6mPSTR11_5.0' 'g6mPSTR12_1.0' 'g6mPSTR13_1.0' 'g6mPSTR13_5.0'\n",
      " 'g6mPSTR14_1.0' 'g6mPSTR15_1.0' 'g6mPSTR16_1.0' 'c6w4muse1_0.0'\n",
      " 'c6w4muse2_0.0' 'c6w4canre_-1.0' 'c6w5muse1_0.0' 'c6w6MUSE1_0.0'\n",
      " 'x6w6MUSE1_0.0' 'c6w6candep_0.0' 'c6dep_0.0' 'c6dep_1.0' 'x6dep_0.0'\n",
      " 'x6dep_1.0' 'parentdx_0.0' 'parentdx_1.0' 'c6w4use_0.0' 'c6w4use_1.0'\n",
      " 'c6w5use_0.0' 'c6w5use_1.0' 'c6w6use_0.0' 'c6w6use_1.0' 'x6w6use_0.0'\n",
      " 'x6w6use_1.0' 'c6use_0.0' 'c6use_1.0' 'x6use_0.0' 'x6use_1.0'\n",
      " 'parentuse_0.0' 'parentuse_1.0' 'ParHist_0.0' 'ParHist_1.0' 'ParHist_2.0'\n",
      " 'ethnic_1.0' 'ethnic_2.0' 'EDUC_7.0' 'EDUC_d_7.0' 'AUD_3Level_0.0'\n",
      " 'AUD_3Level_1.0' 'w6parentdep_0.0' 'w6parentdep_1.0'] \n",
      "\n",
      "Number of variables in each iteration:\n",
      " [97, 97] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.869281045751634, 0.9117647058823529] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.8431372549019608, 0.8431372549019608] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.8363636363636363, 0.8181818181818182]\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestClassifier(random_state=0, bootstrap=True, oob_score=True,\n",
    "                                   class_weight={0:1,1:5,2:20,3:20,4:20,5:20,6:20,7:20})\n",
    "X = x_filled_knn\n",
    "y = y_filtered_df['g6SUUMUSE2']\n",
    "target = 'g6SUUMUSE2'\n",
    "\n",
    "RandFor(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pastYear_use_predictors_tree' (ndarray)\n",
      "Stored 'pastYear_use_scores_tree' (list)\n"
     ]
    }
   ],
   "source": [
    "pastYear_use_predictors_tree = final_var\n",
    "pastYear_use_scores_tree = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store pastYear_use_predictors_tree\n",
    "%store pastYear_use_scores_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past 3 months use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 2 is selected \n",
      "\n",
      "Final model parameters:\t {'min_samples_leaf': 0.01, 'n_estimators': 50} \n",
      "\n",
      "The total number of variables is:\t 87 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['g6SUUcage' 'g6cage' 'g6fpstrmar' 'g6mpstrmar' 'c6w6can4a' 'x6w6can4d'\n",
      " 'par_fscore' 'parf_score2' 'MomPrevent' 'MomDiscuss' 'DadPrevent'\n",
      " 'DadDiscuss' 'DadNegExp' 'g6fparsup' 'g6fparcon' 'g6fparmon' 'g6mparsup'\n",
      " 'g6mparcon' 'g6mparmon' 'c6cage' 'g2agepar' 'g6mcanp01' 'g6mcanp05'\n",
      " 'g6mcanp1' 'g6mcanp2' 'x6mcanp01' 'x6mcanp05' 'x6mcanp1' 'x6mcanp2'\n",
      " 'c6mcanp01' 'c6mcanp05' 'c6mcanp1' 'c6mcanp2' 'x6RELAT_1.0' 'x6RELAT_2.0'\n",
      " 'g6GEN_1.0' 'g6GEN_2.0' 'g6fPSTR9_1.0' 'g6fPSTR9_4.0' 'g6fPSTR10_4.0'\n",
      " 'g6fPSTR11_2.0' 'g6fPSTR12_1.0' 'g6fPSTR12_5.0' 'g6fPSTR13_1.0'\n",
      " 'g6fPSTR14_1.0' 'g6fPSTR14_2.0' 'g6fPSTR14_4.0' 'g6fPSTR16_1.0'\n",
      " 'g6fPSTR16_3.0' 'g6mPSTR10_1.0' 'g6mPSTR10_3.0' 'g6mPSTR10_5.0'\n",
      " 'g6mPSTR11_3.0' 'g6mPSTR12_2.0' 'g6mPSTR13_3.0' 'g6mPSTR14_1.0'\n",
      " 'g6mPSTR14_2.0' 'g6mPSTR16_2.0' 'c6w4muse1_1.0' 'c6w4muse1_2.0'\n",
      " 'c6w4muse2_0.0' 'c6w4candx_1.0' 'c6w4canon_0.0' 'c6w4canre_-2.0'\n",
      " 'c6w5muse3_3.0' 'c6w5canre_6.0' 'c6w6MUSE3_0.0' 'x6w6MUSE1_5.0'\n",
      " 'x6w6MUSE3_1.0' 'c6w6candep_0.0' 'x6w6candep_0.0' 'x6dep_1.0'\n",
      " 'parentdx_1.0' 'c6w5use_0.0' 'c6w6use_0.0' 'x6w6use_1.0' 'x6use_0.0'\n",
      " 'ParHist_0.0' 'ParHist_2.0' 'ethnic_1.0' 'EDUC_3.0' 'EDUC_6.0'\n",
      " 'EDUC_m_7.0' 'EDUC_d_7.0' 'EDUC_d_9.0' 'w6parentdep_0.0'\n",
      " 'w6parentdep_1.0'] \n",
      "\n",
      "Number of variables in each iteration:\n",
      " [92, 87] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.9281045751633987, 0.9379084967320261] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.9052287581699346, 0.9052287581699346] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.9074074074074074, 0.9074074074074074]\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestClassifier(random_state=0, bootstrap=True, oob_score=True,\n",
    "                                   class_weight={0:1,1:2,2:5,3:5,4:5,6:5,7:5})\n",
    "X = x_filled_knn\n",
    "y = y_filtered_df['g6SUUMUSE3']\n",
    "target = 'g6SUUMUSE3'\n",
    "\n",
    "RandFor(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'past3mo_use_predictors_tree' (ndarray)\n",
      "Stored 'past3mo_use_scores_tree' (list)\n"
     ]
    }
   ],
   "source": [
    "past3mo_use_predictors_tree = final_var\n",
    "past3mo_use_scores_tree = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store past3mo_use_predictors_tree\n",
    "%store past3mo_use_scores_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General use (yes or no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model # 4 is selected \n",
      "\n",
      "Final model parameters:\t {'min_samples_leaf': 0.01, 'n_estimators': 10} \n",
      "\n",
      "The total number of variables is:\t 60 \n",
      "\n",
      "These are the variables:\n",
      "\n",
      " ['g6SUUcage' 'g6cage' 'g6fpstrmar' 'g6mpstrmar' 'c6w6can4a' 'x6w6can4d'\n",
      " 'x6w6can4a' 'par_fscore' 'parf_score2' 'MomPrevent' 'MomDiscuss'\n",
      " 'MomNegExp' 'DadPrevent' 'DadDiscuss' 'DadNegExp' 'g6fparsup' 'g6fparcon'\n",
      " 'g6fparmon' 'g6mparsup' 'g6mparcon' 'g6mparmon' 'c6cage' 'g2agepar'\n",
      " 'g6mcanp01' 'g6mcanp1' 'g6mcanp2' 'x6mcanp01' 'x6mcanp05' 'x6mcanp1'\n",
      " 'x6mcanp2' 'c6mcanp01' 'c6mcanp05' 'c6mcanp1' 'c6mcanp2' 'g6GEN_1.0'\n",
      " 'g6GEN_2.0' 'g6fPSTR9_5.0' 'g6fPSTR10_5.0' 'g6fPSTR13_1.0'\n",
      " 'g6fPSTR16_1.0' 'g6mPSTR10_5.0' 'g6mPSTR11_3.0' 'g6mPSTR12_5.0'\n",
      " 'c6w4canon_-1.0' 'c6w4canre_-1.0' 'c6w6MUSE1_0.0' 'c6w6MUSE1_2.0'\n",
      " 'x6w6MUSE1_2.0' 'c6w6candep_1.0' 'x6can4_1.0' 'parentdx_1.0'\n",
      " 'c6w4use_0.0' 'c6use_1.0' 'x6use_1.0' 'parentuse_1.0' 'ParHist_2.0'\n",
      " 'ethnic_1.0' 'EDUC_m_3.0' 'EDUC_d_3.0' 'w6parentdep_0.0'] \n",
      "\n",
      "Number of variables in each iteration:\n",
      " [190, 93, 87, 60, 60] \n",
      "\n",
      "Training accuracy in each iteration:\n",
      " [0.9739413680781759, 0.9478827361563518, 0.9641693811074918, 0.9609120521172638, 0.9641693811074918] \n",
      "\n",
      "Cross-validation accuracy in each iteration:\n",
      " [0.8338762214983714, 0.8403908794788274, 0.8469055374592834, 0.8534201954397395, 0.8403908794788274] \n",
      "\n",
      "Testing accuracy in each iteration:\n",
      " [0.8181818181818182, 0.7454545454545455, 0.7090909090909091, 0.8363636363636363, 0.7636363636363637]\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestClassifier(random_state=0, bootstrap=True, oob_score=True,\n",
    "                                   class_weight={0:1,1:10})\n",
    "X = x_filled_knn\n",
    "y = y_filtered_df['MJ_Outcome_YesNo']\n",
    "target = 'MJ_Outcome_YesNo'\n",
    "\n",
    "RandFor(estimator, X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'gen_use_predictors_tree' (ndarray)\n",
      "Stored 'gen_use_scores_tree' (list)\n"
     ]
    }
   ],
   "source": [
    "gen_use_predictors_tree = final_var\n",
    "gen_use_scores_tree = [train_acc, val_acc, test_acc]\n",
    "\n",
    "%store gen_use_predictors_tree\n",
    "%store gen_use_scores_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = [max_use_scores_regression, pastYear_use_scores_regression,\n",
    "              past3mo_use_scores_regression, gen_use_scores_regression,\n",
    "              max_use_scores_tree, pastYear_use_scores_tree,\n",
    "              past3mo_use_scores_tree, gen_use_scores_tree]\n",
    "\n",
    "score_summary = pd.DataFrame(columns=['Training Accuracy', 'Cross-Validation Accuracy', 'Testing Accuracy'], \n",
    "                             data=all_scores)\n",
    "\n",
    "response = [\"Max Use\", \"Past Year Use\", \"Past 3 Months Use\", \"General Use\"]\n",
    "response = response + response\n",
    "model = [\"Logistic Regression\", \"Random Forest\"]\n",
    "model = np.repeat(model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Cross-Validation Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Max Use</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.944625</td>\n",
       "      <td>0.726759</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Past Year Use</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.823610</td>\n",
       "      <td>0.836364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Past 3 Months Use</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>0.882418</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Use</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.954397</td>\n",
       "      <td>0.863052</td>\n",
       "      <td>0.836364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Max Use</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986971</td>\n",
       "      <td>0.771987</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Past Year Use</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.869281</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.836364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Past 3 Months Use</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.937908</td>\n",
       "      <td>0.905229</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>General Use</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.960912</td>\n",
       "      <td>0.853420</td>\n",
       "      <td>0.836364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Variable                Model  Training Accuracy  \\\n",
       "0            Max Use  Logistic Regression           0.944625   \n",
       "1      Past Year Use  Logistic Regression           0.941176   \n",
       "2  Past 3 Months Use  Logistic Regression           0.967320   \n",
       "3        General Use  Logistic Regression           0.954397   \n",
       "4            Max Use        Random Forest           0.986971   \n",
       "5      Past Year Use        Random Forest           0.869281   \n",
       "6  Past 3 Months Use        Random Forest           0.937908   \n",
       "7        General Use        Random Forest           0.960912   \n",
       "\n",
       "   Cross-Validation Accuracy  Testing Accuracy  \n",
       "0                   0.726759          0.690909  \n",
       "1                   0.823610          0.836364  \n",
       "2                   0.882418          0.870370  \n",
       "3                   0.863052          0.836364  \n",
       "4                   0.771987          0.727273  \n",
       "5                   0.843137          0.836364  \n",
       "6                   0.905229          0.907407  \n",
       "7                   0.853420          0.836364  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_summary.insert(0, \"Model\", model, allow_duplicates = True)\n",
    "score_summary.insert(0, \"Variable\", response, allow_duplicates = True)\n",
    "score_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
